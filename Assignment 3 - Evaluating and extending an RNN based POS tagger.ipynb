{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 3 - RNN based POS tagger.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgPqt4p1y36p"
      },
      "source": [
        "# Assignment 3: Evaluating and extending an RNN based POS tagger\n",
        " \n",
        "*version 2021.1, details/bugs might be fixed within the first week of publication, you will be notified if this happens*\n",
        " \n",
        "When solving a new problem (or an old problem on a new data set), there are often some \"shoulders of giants\" to stand on. The first step is to look at the best papers and try to download someone's code for solving a similar problem to the one you are faced with. The challenge is as much about understanding and adapting already existing code as it is about theoretical knowledge and creativity.\n",
        " \n",
        "For this assignment, you start with a working pipeline for POS tagging using LSTMs and mini-batch training. Your task is to evaluate the model on a number of sources in different languages. The neural model represents the data's tokens as non-pretrained embedding vectors. The sequences of embedding vectors are passed through a LSTM layer. The outputs from the recurrent layer are then transformed to probabilities over POS tags by passing them through a fully connected layer and a softmax. You will have to refactor (i.e. rearrange the code) and extend the model by adding some commonly used properties, e.g. the RNN layer type or regularization (see the list of suggestions below).\n",
        " \n",
        "The familiar Brown corpus is used for the reference implementation. This corpus should not be used in your submission.\n",
        "\n",
        "## Submission\n",
        " \n",
        "Please submit your code as a notebook through studium. You should include the following:\n",
        " \n",
        "1. A working implementation of your pipeline, reproducing your principal results when run. Please ensure that outputs are stored in the notebook (preferably by rerunning your notebook as the last thing you do before submitting).\n",
        "2. Comment the code properly, especially for longer or opaque functionality. Please try to write self documenting code (e.g. by choosing descriptive variables names, refactoring to isolate functionality, and by minimizing code duplication).\n",
        "4. Comments on what you thought was hard in the assignment, what you think was educational, what took most time, and which parts might be unnecessarily tricky.\n",
        "5. As the submission is anonymous, **all personal information must be removed**.\n",
        "6. Apart from sections and titles, please remove all unnecessary text and code from the notebook you hand in. Keep only that which strengthens the case that you fulfil the listed requirements.\n",
        "\n",
        "\n",
        "## Requirements for grade G\n",
        "\n",
        "To achieve a pass (G) on this assignment, you must solve the following tasks without serious errors.\n",
        "\n",
        "1. Extend the given model in at least three of the ways listed in the *extensions* section. State clearly which ones you have implemented.\n",
        "2. Include a short (250-500 words) qualitative analysis in your submission. Discuss the performance difference between languages/genres and design choices. You can spread this in the notebook or put them in one place.\n",
        "3. Refactor the given model as a class (including relevant preprocessing, forward/backward pass etc). Your model class should follow the sklearn API where possible.\n",
        "4. Not all languages require the same model complexity for POS tagging. Some might need a higher dimensionality for the embedding or layers for the model to perform well. Briefly explore the relation between model complexity and accuracy?\n",
        "5. In your quantitative evaluation, use some genre of text in three languages from the [universal dependencies](https://universaldependencies.org/) project (UD). These will be in the familiar `.conllu` format. Parse the files to get the words (not lemmas).\n",
        "6. Network accuracy for one model doesn't say much about how well a model performs. Define a baseline accuracy and compare your results to it.\n",
        "\n",
        "### Additional requirements for grade VG\n",
        "\n",
        "For a pass with distinction grade (VG), you should extend your analysis to include all the items under *extensions*. State clearly which ones you have done.\n",
        "\n",
        "\n",
        "### Extensions \n",
        " \n",
        "1. There are other types of RNNs layers commonly used in NLP. Add the option to use a GRU layer instead of LSTM, and include this in your performance comparison.\n",
        "2. The given implementation only allows for dependencies from left to right. Add the option to use a bi-directional RNN layer.\n",
        "3. Use pyTorch's `Dataset` and `DataLoader` classes for loading the data. [This tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) is a good starting point. This should make it easier to loop over data sets.\n",
        "4. Try some type of data augmentation in your training data (e.g. masking random tokens). This should theoretically increase the generalizability of your model. How much augmentation is too much?\n",
        "5. Implement some level of regularization in your model. This can be implemented in several ways, e.g. dropout or weight decay. Briefly argue for the choices you made and show how network preformance might change with the rate of regularization.\n",
        "6. Add more sources. You can both add sources with different genres and/or more languages. A suggestion is to try how languages with very different levels of morphological richness (e.g. English vs Finish) requires more or less training data. The total number of sources should be above 10.\n",
        "7. Comment on shortcomings and how to improve the model in relation to a recent POS tagging paper (a good starting point for searching is [NLP-progress](http://nlpprogress.com/).\n",
        "10. Compare performace using UD's universal vs extended tag sets.\n",
        "\n",
        "*Note that trying all combinations of the extensions is not required. It is enough to do some structured testing of extensions and then, for example, go on to compare the tag sets on the best model configuration. This is an exercise in extending and evaluating a model, not in finding the patience to wait for your computer to finish grid searching over alternatives.*\n",
        "\n",
        "## General advice\n",
        "\n",
        "The task is to predict, for an unseen review, whether it is positive or negative. This is a binary classification task. Work from the given code and change one piece at a time, ensuring functionality throughout your work. You should test your code every couple of lines to make sure your assumptions on functionality and variable content is correct. A good rule of thumb is that a coder will introduce a bug every five lines (even as a professional).\n",
        "\n",
        "To make sure your code does what it is supposed to do, use ```assert``` statements to check your assumptions. Keep the given asserts if you need them. Professional coders sometimes start with writing tests for some functionality instead of starting with the functionality itself. This is called *test-driven development*.\n",
        "\n",
        "Finally, remember the great motto **RTFM**. It will save you in the future.\n",
        "\n",
        "## Plagiarism\n",
        " \n",
        "In code assignments, plagiarism is a tricky concept. A clean cut way would be to demand that you write all the code yourself, from memory, with only the assigned literature as help. This is not how code is developed professionally. It is common to copy and share. However, since this is a learning exercise, you must implement everything on your own, but please look at the course repo, Stack Overflow etc. Moreover, discuss with course mates and TAs to find inspiration and solutions. Code that is *obviously* copied (with minor modifications) will be considered as plagiarized. As a part of the examination, you might be asked to explain any particular part of the functionality in your implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDDstzhwztQS"
      },
      "source": [
        "# Our standard imports for maths and basic methodology\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For user feedback\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Imports for pytorch\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Okj6Den-4-Wd"
      },
      "source": [
        "Let's see if we have a GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYGiUxG746pz",
        "outputId": "3c581261-7c26-47f7-bb03-90081571d691"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  for i in range(torch.cuda.device_count()):\n",
        "    print(torch.cuda.get_device_name(i))\n",
        "else:\n",
        "  print(\"No GPU available\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO0m41-MzJRc"
      },
      "source": [
        "## Load tagging data\n",
        "\n",
        "The following downloads the [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus). This data is only here to demonstrate the network below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l67a4Qnr5ac7",
        "outputId": "8af4a468-96a6-40ce-d186-bdf6a59238e7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "sentences = brown.tagged_sents(tagset='universal')                        # Load the data\n",
        "sentences = [sentence for sentence in sentences if len(sentence) > 2]     # Remove very short sentences\n",
        "\n",
        "print(\"Loaded %i sentences\" % len(sentences))\n",
        "print(sentences[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "Loaded 56283 sentences\n",
            "[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOQVXabf6MH9"
      },
      "source": [
        "Preprocessing for the brow corpus. This splits the data into our standard X and y format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UuA4rJC6Hge",
        "outputId": "20dee52b-053e-421a-95e2-edeb1e396857"
      },
      "source": [
        "X = [[token for token, tag in sentence] for sentence in sentences]\n",
        "y = [[tag for token, tag in sentence] for sentence in sentences]\n",
        "\n",
        "assert len(X) == len(y)\n",
        "\n",
        "print(X[0])\n",
        "print(y[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
            "['DET', 'NOUN', 'NOUN', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'VERB', '.', 'DET', 'NOUN', '.', 'ADP', 'DET', 'NOUN', 'VERB', 'NOUN', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SKxeDs2seHV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51daa35c-6639-4571-d7c8-254e52d0d986"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
        "\n",
        "assert len(X_train) == len(y_train)\n",
        "assert len(X_test) == len(y_test)\n",
        "assert len(X_train)+len(X_test) == len(X)\n",
        "\n",
        "print(\"The training set includes %i sentences\" % len(X_train))\n",
        "print(\"The test set includes %i sentences\" % len(X_test))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training set includes 50654 sentences\n",
            "The test set includes 5629 sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlL3-sS57GeE"
      },
      "source": [
        "Most sentences are short, but some are very long."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z25i9_UxtUeR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "5e1fbdd1-4cc8-435f-cefa-ea142703c429"
      },
      "source": [
        "l = np.asarray([len(x) for x in X], dtype=np.int)\n",
        "plt.figure(figsize=(8, 4))\n",
        "x = np.unique(l)\n",
        "plt.bar(x, [np.sum(l==e) for e in x], width=1)\n",
        "plt.xlabel(\"Sentence length\")\n",
        "plt.ylabel(\"# sentences\")\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHIiYxLyz2cf"
      },
      "source": [
        "## Data encoding and padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fjVkyDOP9MU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10c0eec5-4c0e-4631-d2c6-a08ad3074e08"
      },
      "source": [
        "tokens = {token for sentence in X_train for token in sentence}\n",
        "idx2token = list(tokens)\n",
        "idx2token.insert(0, '<UNK>')\n",
        "idx2token.append('<PAD>')\n",
        "token2idx = {token:idx for idx, token in enumerate(idx2token)}\n",
        "\n",
        "tags = {tag for tags in y_train for tag in tags}\n",
        "idx2tag = list(tags)\n",
        "idx2tag.append('<PAD>')\n",
        "tag2idx = {tag:idx for idx, tag in enumerate(idx2tag)}\n",
        "\n",
        "print(idx2token[:15])\n",
        "print(idx2tag)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<UNK>', 'weddings', '250,000', 'humanness', 'lobscouse', 'in-fighting', 'outweighed', 'tiny', 'brick', 'mitigate', 'retold', 'politically', 'Pennock', 'hops', 'labeling']\n",
            "['NOUN', 'CONJ', 'PRON', 'ADP', 'ADV', 'VERB', 'X', 'NUM', 'PRT', 'DET', '.', 'ADJ', '<PAD>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdgiIDVP9Brw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58688c76-f7bb-4205-9c41-c7a7dc00587c"
      },
      "source": [
        "def pad_and_encode(sentences, labels):\n",
        "  assert len(sentences)==len(labels)\n",
        "  assert np.all([len(sentence)==len(tags) for sentence, tags in zip(sentences, labels)])\n",
        "  max_sentence_length = np.max([len(sentence) for sentence in sentences]) # Find out how much to pad\n",
        "  padded_sentences = torch.zeros(len(sentences), max_sentence_length,     # Create data structures with <PAD> as default\n",
        "                                 dtype=torch.long)\n",
        "  padded_sentences[:] = token2idx['<PAD>']\n",
        "  padded_labels = torch.zeros(len(sentences), max_sentence_length, \n",
        "                              dtype=torch.long)\n",
        "  padded_labels[:] = tag2idx['<PAD>']\n",
        "  for i, (sentence, tags) in enumerate(zip(sentences, labels)):               # Loop over the data\n",
        "    for j, token in enumerate(sentence):\n",
        "      if token in token2idx.keys():\n",
        "        padded_sentences[i, j] = token2idx[token]\n",
        "      else:\n",
        "        padded_sentences[i, j] = token2idx['<UNK>']\n",
        "    for j, tag in enumerate(tags):\n",
        "      padded_labels[i, j] = tag2idx[tag]\n",
        "  return padded_sentences, padded_labels\n",
        "\n",
        "a, b = pad_and_encode(X_train[:5], y_train[:5])\n",
        "print(a)\n",
        "print(b)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[28012, 18944, 39031, 27869, 50460, 52344, 13557, 11286, 33402, 43480,\n",
            "         13557, 11286, 20241, 13557, 43497, 52852,  4990, 53337, 53337, 53337,\n",
            "         53337, 53337, 53337, 53337, 53337, 53337, 53337, 53337, 53337],\n",
            "        [47409,  8246,  3425, 11196,  8152, 50970, 23316, 47312, 48923, 36942,\n",
            "         39502, 21898,  3425, 38568, 23879, 23178, 23316, 36084, 48923, 24569,\n",
            "          9214, 22261, 12044,  1266, 53256, 13557, 23178, 49926,  4990],\n",
            "        [34792, 34156, 38288,  3370, 38568, 41839, 31996,  3425,   988, 33325,\n",
            "           988, 34792, 35860, 52390,  4990, 53337, 53337, 53337, 53337, 53337,\n",
            "         53337, 53337, 53337, 53337, 53337, 53337, 53337, 53337, 53337],\n",
            "        [25340, 53148,  9815, 32599, 38568, 13432, 53148,  9815, 36894,  4990,\n",
            "         53337, 53337, 53337, 53337, 53337, 53337, 53337, 53337, 53337, 53337,\n",
            "         53337, 53337, 53337, 53337, 53337, 53337, 53337, 53337, 53337],\n",
            "        [ 5446, 48343, 11286, 42007,  3425, 48343, 11286, 38552,  3425, 21323,\n",
            "         11286, 28292,  3425, 51708, 38568, 13882,  3425, 38568, 21981, 19200,\n",
            "         19311, 38745, 41541, 11286, 45112, 45535, 23178, 12664,  4990]])\n",
            "tensor([[ 9,  0,  5,  9, 11,  0,  3,  9, 11,  0,  3,  9,  0,  3,  9, 11, 10, 12,\n",
            "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12],\n",
            "        [ 3,  8, 10,  0,  5,  9, 10, 11, 10,  1, 11,  0, 10,  1,  4,  9, 10, 11,\n",
            "         10,  0,  5,  5,  5,  3,  9,  3,  9,  0, 10],\n",
            "        [ 2,  5, 11,  5,  1,  5,  4, 10,  4, 11,  3,  2,  5,  4, 10, 12, 12, 12,\n",
            "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12],\n",
            "        [ 0,  5,  2,  5,  1,  0,  5,  2,  4, 10, 12, 12, 12, 12, 12, 12, 12, 12,\n",
            "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12],\n",
            "        [ 2,  5,  9,  0, 10,  5,  9,  0, 10,  5,  9,  0, 10,  0,  1,  5, 10,  1,\n",
            "          4,  5,  9,  0,  3,  9,  0,  3,  9,  0, 10]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbZWgBRAGWQa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2db96463-ef55-4e1e-e140-224b00006bc6"
      },
      "source": [
        "def batch_iterator(sentences, labels, batch_size=64):\n",
        "  \"\"\"Helper function for iterating over batches of the data\"\"\"\n",
        "  assert len(sentences) == len(labels)\n",
        "  for i in range(0, len(sentences), batch_size):\n",
        "    X, y = pad_and_encode(sentences[i:min(i+batch_size, len(sentences))], \n",
        "                          labels[i:min(i+batch_size, len(sentences))])\n",
        "    if torch.cuda.is_available():                                               # Move data to the GPU, if possible, before yielding it\n",
        "      yield (X.cuda(), y.cuda())\n",
        "    else:\n",
        "      yield (X, y)\n",
        "\n",
        "next(batch_iterator(X_train, y_train, batch_size=5))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[28012, 18944, 39031, 27869, 50460, 52344, 13557, 11286, 33402, 43480,\n",
              "          13557, 11286, 20241, 13557, 43497, 52852,  4990, 53337, 53337, 53337,\n",
              "          53337, 53337, 53337, 53337, 53337, 53337, 53337, 53337, 53337],\n",
              "         [47409,  8246,  3425, 11196,  8152, 50970, 23316, 47312, 48923, 36942,\n",
              "          39502, 21898,  3425, 38568, 23879, 23178, 23316, 36084, 48923, 24569,\n",
              "           9214, 22261, 12044,  1266, 53256, 13557, 23178, 49926,  4990],\n",
              "         [34792, 34156, 38288,  3370, 38568, 41839, 31996,  3425,   988, 33325,\n",
              "            988, 34792, 35860, 52390,  4990, 53337, 53337, 53337, 53337, 53337,\n",
              "          53337, 53337, 53337, 53337, 53337, 53337, 53337, 53337, 53337],\n",
              "         [25340, 53148,  9815, 32599, 38568, 13432, 53148,  9815, 36894,  4990,\n",
              "          53337, 53337, 53337, 53337, 53337, 53337, 53337, 53337, 53337, 53337,\n",
              "          53337, 53337, 53337, 53337, 53337, 53337, 53337, 53337, 53337],\n",
              "         [ 5446, 48343, 11286, 42007,  3425, 48343, 11286, 38552,  3425, 21323,\n",
              "          11286, 28292,  3425, 51708, 38568, 13882,  3425, 38568, 21981, 19200,\n",
              "          19311, 38745, 41541, 11286, 45112, 45535, 23178, 12664,  4990]],\n",
              "        device='cuda:0'),\n",
              " tensor([[ 9,  0,  5,  9, 11,  0,  3,  9, 11,  0,  3,  9,  0,  3,  9, 11, 10, 12,\n",
              "          12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12],\n",
              "         [ 3,  8, 10,  0,  5,  9, 10, 11, 10,  1, 11,  0, 10,  1,  4,  9, 10, 11,\n",
              "          10,  0,  5,  5,  5,  3,  9,  3,  9,  0, 10],\n",
              "         [ 2,  5, 11,  5,  1,  5,  4, 10,  4, 11,  3,  2,  5,  4, 10, 12, 12, 12,\n",
              "          12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12],\n",
              "         [ 0,  5,  2,  5,  1,  0,  5,  2,  4, 10, 12, 12, 12, 12, 12, 12, 12, 12,\n",
              "          12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12],\n",
              "         [ 2,  5,  9,  0, 10,  5,  9,  0, 10,  5,  9,  0, 10,  0,  1,  5, 10,  1,\n",
              "           4,  5,  9,  0,  3,  9,  0,  3,  9,  0, 10]], device='cuda:0'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmgbWbsJk8DW"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwU6cWT2AqT6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "170f6273-67b6-4dda-b17d-e47eabaa03dd"
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "  def __init__(self, word_embedding_dim, lstm_hidden_dim, vocabulary_size, tagset_size):\n",
        "    \"\"\"An LSTM based tagger\n",
        "    \n",
        "    word_embedding_dim\n",
        "      The dimensionality of the word embedding\n",
        "    lstm_hidden_dim\n",
        "      The dimensionality of the hidden state in the LSTM\n",
        "    vocabulary_size\n",
        "      The number of unique tokens in the word embedding (including <PAD> etc)\n",
        "    tagset_size\n",
        "      The number of unique POS tags (not including <PAD>, as we don't want to predict it)\n",
        "    \"\"\"\n",
        "    super(LSTMTagger, self).__init__()                                          # We need to initialise the class we are inheriting from\n",
        "    self.lstm_hidden_dim_ = lstm_hidden_dim                                     # This simply stores the parameters\n",
        "    self.vocabulary_size_ = vocabulary_size\n",
        "    self.tagset_size_ = tagset_size\n",
        "\n",
        "    self._word_embedding = nn.Embedding(num_embeddings=vocabulary_size,         # Creates the vector space for the input words\n",
        "                                         embedding_dim=word_embedding_dim, \n",
        "                                         padding_idx=token2idx['<PAD>'])\n",
        "    self._lstm = nn.LSTM(input_size=word_embedding_dim,                         # The LSTM takes an embedded sentence as input, and outputs \n",
        "                         hidden_size=lstm_hidden_dim,                           # vectors with dimensionality lstm_hidden_dim.\n",
        "                         batch_first=True)\n",
        "    self._fc = nn.Linear(lstm_hidden_dim, tagset_size)                          # The linear layer maps from the RNN output space to tag space\n",
        "    self._softmax = nn.LogSoftmax(dim=1)                                        # Softmax of outputting PDFs over tags\n",
        "    \n",
        "    self.training_loss_ = list()                                                # For plotting\n",
        "    self.training_accuracy_ = list()\n",
        "\n",
        "    if torch.cuda.is_available():                                               # Move the model to the GPU (if we have one)\n",
        "      self.cuda()\n",
        "\n",
        "  def forward(self, padded_sentences):\n",
        "    \"\"\"The forward pass through the network\"\"\"\n",
        "    batch_size, max_sentence_length = padded_sentences.size()\n",
        "\n",
        "    embedded_sentences = self._word_embedding(padded_sentences)                 # Sentences encoded as integers are mapped to vectors    \n",
        "\n",
        "    sentence_lengths = (padded_sentences!=token2idx['<PAD>']).sum(dim=1)        # Find the length of sentences\n",
        "    sentence_lengths = sentence_lengths.long().cpu()                            # Ensure the correct format\n",
        "    X = nn.utils.rnn.pack_padded_sequence(embedded_sentences, sentence_lengths, # Pack the embedded data\n",
        "                                          batch_first=True, enforce_sorted=False)\n",
        "    lstm_out, _ = self._lstm(X)                                                 # Run the LSTM layer\n",
        "    X, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)         # Unpack the output from the LSTM\n",
        "\n",
        "    X = X.contiguous().view(-1, X.shape[2])                                     # The output from the LSTM layer is flattened\n",
        "    tag_space = self._fc(X)                                                     # Fully connected layer\n",
        "    tag_scores = self._softmax(tag_space)                                       # Softmax is applied to normalise the outputs\n",
        "    return tag_scores.view(batch_size, max_sentence_length, self.tagset_size_)\n",
        "\n",
        "\n",
        "model = LSTMTagger(word_embedding_dim=32,                                       # Dimensionality of the work embedding\n",
        "                   lstm_hidden_dim=64,                                          # Dimensionality of the hidden state in the LSTM\n",
        "                   vocabulary_size=len(token2idx),                              # The vocabulary incudes both the 'padding' and 'unknown' symbols\n",
        "                   tagset_size=len(tag2idx)-1)                                  # We have no interest in the network outputting the padding symbol\n",
        "print(model)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTMTagger(\n",
            "  (_word_embedding): Embedding(53338, 32, padding_idx=53337)\n",
            "  (_lstm): LSTM(32, 64, batch_first=True)\n",
            "  (_fc): Linear(in_features=64, out_features=12, bias=True)\n",
            "  (_softmax): LogSoftmax(dim=1)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEZlbcallbff"
      },
      "source": [
        "## Network training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVBW4NNNOwY5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4529a1e5-d1a7-4d38-ab60-1e82c849a840"
      },
      "source": [
        "loss_function = nn.NLLLoss(ignore_index=tag2idx['<PAD>'])                       # A loss function that fits our choice of output layer and data. The\n",
        "                                                                                # loss function will ignore the padding index in the targets.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)                                # We give the optimiser the parameters to work with, note that we can choose to only give some parameters\n",
        "\n",
        "batch_size = 256                                                                # Define the size of each batch\n",
        "for epoch in range(5):                                                          # Times to loop over the full dataset\n",
        "  with tqdm(batch_iterator(X_train, y_train, batch_size=batch_size), \n",
        "            total=len(X_train)//batch_size+1, unit=\"batch\", desc=\"Epoch %i\" % epoch) as batches:\n",
        "    for inputs, targets in batches:                                             # Loop once over the training data\n",
        "      model.zero_grad()                                                         # Reset gradients\n",
        "      scores = model(inputs)                                                    # Forward pass\n",
        "      loss = loss_function(scores.view(-1, model.tagset_size_),                 # Get loss, the data is reshaped as a long line of predictions and targets\n",
        "                           targets.view(-1))               \n",
        "      loss.backward()                                                           # Backpropagate the error\n",
        "      optimizer.step()                                                          # Run the optimizer to change the weights w.r.t the loss\n",
        "      predictions = scores.argmax(dim=2, keepdim=True).squeeze()                # Calculate the batch training accuracy\n",
        "      mask = targets!=tag2idx['<PAD>']                                          # Create a mask for ignoring <PAD> in the targets\n",
        "      correct = (predictions[mask] == targets[mask]).sum().item()               # Item pulls the value from the GPU automatically (if needed)\n",
        "      accuracy = correct / mask.sum().item()*100\n",
        "      model.training_accuracy_.append(accuracy)                                 # Save the accuracy for plotting\n",
        "      model.training_loss_.append(loss.item())                                  # Save the loss for plotting\n",
        "      batches.set_postfix(loss=loss.item(), accuracy=accuracy)                  # Update the progress bar"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 198/198 [00:12<00:00, 16.46batch/s, accuracy=94, loss=0.176]\n",
            "Epoch 1: 100%|██████████| 198/198 [00:11<00:00, 16.57batch/s, accuracy=96.3, loss=0.0962]\n",
            "Epoch 2: 100%|██████████| 198/198 [00:12<00:00, 16.34batch/s, accuracy=97.3, loss=0.0717]\n",
            "Epoch 3: 100%|██████████| 198/198 [00:12<00:00, 16.41batch/s, accuracy=97.5, loss=0.0619]\n",
            "Epoch 4: 100%|██████████| 198/198 [00:12<00:00, 16.27batch/s, accuracy=97.9, loss=0.0551]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSf4AVZislgg"
      },
      "source": [
        "We can plot the stored loss over epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6rd8T0_q24D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "b2142c51-8628-4ced-ae81-936cab48644c"
      },
      "source": [
        "fig = plt.figure(figsize=(6, 4))\n",
        "ax = plt.subplot()\n",
        "ax.set_title(\"Plot for the (hopefully) decreasing loss over epochs\")\n",
        "ax.plot(model.training_loss_, 'b-')\n",
        "ax.set_ylabel(\"Training Loss\", color='b')\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "# ax.set_yscale('log')\n",
        "ax.tick_params(axis='y', labelcolor='b')\n",
        "ax = ax.twinx()\n",
        "ax.plot(model.training_accuracy_, 'r-')\n",
        "ax.set_ylabel(\"Accuracy [%]\", color='r')\n",
        "ax.tick_params(axis='y', labelcolor='r')\n",
        "a = list(ax.axis())\n",
        "a[2] = 0\n",
        "a[3] = 100\n",
        "ax.axis(a)\n",
        "t = np.arange(0, len(model.training_accuracy_), len(X_train)//batch_size+1)\n",
        "ax.set_xticks(ticks=t)\n",
        "ax.set_xticklabels(labels=np.arange(len(t)))\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUDy6CJKcMSh"
      },
      "source": [
        "## Test data accuracy\n",
        "\n",
        "This shows that the model sort of works. A per sentence accuracy would be better while being able to also analyse the predictions qualitatively would be best."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48fkSBgEBnFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "712695eb-cada-4a26-a29b-3c3844ceaf1c"
      },
      "source": [
        "with torch.no_grad():                                                           # Do not use the following forward passes to calculate a gradient\n",
        "  n_correct = 0\n",
        "  n_total = 0\n",
        "  for inputs, targets in batch_iterator(X_test, y_test, batch_size=batch_size): # Loop once over the test data\n",
        "    scores = model(inputs)                                                      # Runs the test data through the model\n",
        "    predictions = scores.argmax(dim=2, keepdim=True).squeeze()                  # Finds the predictions\n",
        "    mask = targets!=tag2idx['<PAD>']                                            # Create a mask for ignoring <PAD> in the targets\n",
        "    n_correct += (predictions[mask] == targets[mask]).sum().item()              # Sums the number of correct predictions\n",
        "    n_total += mask.sum().item()\n",
        "print(\"Test accuracy %.1f%%\" % (100*n_correct/n_total))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy 96.1%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}